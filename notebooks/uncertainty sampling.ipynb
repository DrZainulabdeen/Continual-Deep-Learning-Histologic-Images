{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy.stats import entropy\n","\n","a = np.random.random_integers(0, 9, (10, 10))\n","b = np.random.random_integers(0, 9, (10, 10))\n","c = np.random.random_integers(0, 9, (10, 10))"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n array([17,  7,  8,  8, 11,  9,  7, 12, 15,  6]))"},"metadata":{},"execution_count":32}],"source":["np.unique(a, return_counts=True)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([[1.7, 0.7, 0.8, 0.8, 1.1, 0.9, 0.7, 1.2, 1.5, 0.6],\n       [1.1, 0.6, 1.7, 1.4, 0.9, 1. , 1.1, 0.2, 0.9, 1.1],\n       [1. , 0.9, 1.2, 0.6, 0.9, 0.5, 1.2, 1.7, 0.9, 1.1]])"},"metadata":{},"execution_count":59}],"source":["_ , countsA = np.unique(a, return_counts=True)\n","_ , countsB = np.unique(b, return_counts=True)\n","_ , countsC = np.unique(c, return_counts=True)\n","counts = np.array([countsA,countsB, countsC])\n","class_probs = np.array(pred_probability(counts))\n","class_probs"]},{"cell_type":"markdown","metadata":{},"source":["# Entropy"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([2.24496526, 2.21562301, 2.25189123])"},"metadata":{},"execution_count":60}],"source":["#Entropy using Scipy and Pandas\n","entropy(counts.T)"]},{"cell_type":"markdown","metadata":{},"source":["Here entropy is very high more than 1\n","this shows very high disoder. Normal range of entropy is 0-1"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([2.24496526, 2.21562301, 2.25189123])"},"metadata":{},"execution_count":62}],"source":["#Entropy using Scipy and probability function\n","\n","calss_probs = pred_probability(counts)\n","entropy(class_probs.T)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["\n","def pred_probability(counts):\n","    prob = []\n","    for sample in counts:\n","        n_classes = len(sample)\n","        prob.append(sample/n_classes)\n","    return prob"]},{"cell_type":"markdown","metadata":{},"source":["# Classification Uncertainty"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([-0.7, -0.7, -0.7])"},"metadata":{},"execution_count":64}],"source":["''' This approach selects the 1 most uncertain sample. Large number represents uncertainty. We should pass multiple lists of probs\n","instead of 1 sample to get a list of uncertainities and select the one with highest number. In this case pass axis = 1 for max function '''\n","\n","1 - class_probs.max(axis = 1) "]},{"cell_type":"markdown","metadata":{},"source":["# Margin Sampling"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([0.2, 0.3, 0.5])"},"metadata":{},"execution_count":65}],"source":["''' Considers the margin between first and second most likely prediction. It selects the sample with smallest margin, the smaller the margin between the prediction of two classes the more unsure the decision is. '''\n","\n","part = np.partition(-class_probs, 1, axis=1)\n","margin = - part[:, 0] + part[:, 1]\n","margin"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2-final"},"orig_nbformat":2,"kernelspec":{"name":"python38264bit7b4bc241a53541bfa8e798fdaad69a89","display_name":"Python 3.8.2 64-bit"}},"nbformat":4,"nbformat_minor":2}